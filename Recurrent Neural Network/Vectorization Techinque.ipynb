{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35982e6d",
   "metadata": {},
   "source": [
    "# <center>Vectorization Techinque</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c91e0",
   "metadata": {},
   "source": [
    "# 1. Integer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58629071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence=['Hi I am Victor',\n",
    "         'This is RNN',\n",
    "         'I am Invisible but watching',\n",
    "         'Who am I',\n",
    "         'Now testing for Vectorization Technique',\n",
    "         'Which is of two type: Integer Extraction',\n",
    "          'Second one is Embedding',\n",
    "         'Today I ate croissant',\n",
    "          'Thanks for watching',\n",
    "          'Have a nice day'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "701334e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer(oov_token='none') #oov_token: when unknow word will tackeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5bcf0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate Index\n",
    "'''\n",
    "tokenizer.fit_on_texts(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622e3a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'none': 1,\n",
       " 'i': 2,\n",
       " 'am': 3,\n",
       " 'is': 4,\n",
       " 'watching': 5,\n",
       " 'for': 6,\n",
       " 'hi': 7,\n",
       " 'victor': 8,\n",
       " 'this': 9,\n",
       " 'rnn': 10,\n",
       " 'invisible': 11,\n",
       " 'but': 12,\n",
       " 'who': 13,\n",
       " 'now': 14,\n",
       " 'testing': 15,\n",
       " 'vectorization': 16,\n",
       " 'technique': 17,\n",
       " 'which': 18,\n",
       " 'of': 19,\n",
       " 'two': 20,\n",
       " 'type': 21,\n",
       " 'integer': 22,\n",
       " 'extraction': 23,\n",
       " 'second': 24,\n",
       " 'one': 25,\n",
       " 'embedding': 26,\n",
       " 'today': 27,\n",
       " 'ate': 28,\n",
       " 'croissant': 29,\n",
       " 'thanks': 30,\n",
       " 'have': 31,\n",
       " 'a': 32,\n",
       " 'nice': 33,\n",
       " 'day': 34}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8437b738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hi', 1),\n",
       "             ('i', 4),\n",
       "             ('am', 3),\n",
       "             ('victor', 1),\n",
       "             ('this', 1),\n",
       "             ('is', 3),\n",
       "             ('rnn', 1),\n",
       "             ('invisible', 1),\n",
       "             ('but', 1),\n",
       "             ('watching', 2),\n",
       "             ('who', 1),\n",
       "             ('now', 1),\n",
       "             ('testing', 1),\n",
       "             ('for', 2),\n",
       "             ('vectorization', 1),\n",
       "             ('technique', 1),\n",
       "             ('which', 1),\n",
       "             ('of', 1),\n",
       "             ('two', 1),\n",
       "             ('type', 1),\n",
       "             ('integer', 1),\n",
       "             ('extraction', 1),\n",
       "             ('second', 1),\n",
       "             ('one', 1),\n",
       "             ('embedding', 1),\n",
       "             ('today', 1),\n",
       "             ('ate', 1),\n",
       "             ('croissant', 1),\n",
       "             ('thanks', 1),\n",
       "             ('have', 1),\n",
       "             ('a', 1),\n",
       "             ('nice', 1),\n",
       "             ('day', 1)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35afafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 2, 3, 8],\n",
       " [9, 4, 10],\n",
       " [2, 3, 11, 12, 5],\n",
       " [13, 3, 2],\n",
       " [14, 15, 6, 16, 17],\n",
       " [18, 4, 19, 20, 21, 22, 23],\n",
       " [24, 25, 4, 26],\n",
       " [27, 2, 28, 29],\n",
       " [30, 6, 5],\n",
       " [31, 32, 33, 34]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Creating Sequence of word\n",
    "'''\n",
    "sequence=tokenizer.texts_to_sequences(sentence)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1982b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7990254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  2,  3,  8,  0,  0,  0],\n",
       "       [ 9,  4, 10,  0,  0,  0,  0],\n",
       "       [ 2,  3, 11, 12,  5,  0,  0],\n",
       "       [13,  3,  2,  0,  0,  0,  0],\n",
       "       [14, 15,  6, 16, 17,  0,  0],\n",
       "       [18,  4, 19, 20, 21, 22, 23],\n",
       "       [24, 25,  4, 26,  0,  0,  0],\n",
       "       [27,  2, 28, 29,  0,  0,  0],\n",
       "       [30,  6,  5,  0,  0,  0,  0],\n",
       "       [31, 32, 33, 34,  0,  0,  0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Adding Padding i.e, [5,2,3,0,0,0,0]\n",
    "'''\n",
    "sequence=pad_sequences(sequence,padding='post')\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f277d",
   "metadata": {},
   "source": [
    "# 2. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12908e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 7, 2)              66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66\n",
      "Trainable params: 66\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Adding Embedding Layer\n",
    "'''\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding\n",
    "model=Sequential()\n",
    "model.add(Embedding(33,output_dim=2,input_length=7))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a00cd511",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam','accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
